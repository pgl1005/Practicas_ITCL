{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Red Neuronal según los principios de Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id  patient_nbr             race  gender      age weight  \\\n",
      "0       2278392      8222157        Caucasian  Female   [0-10)      ?   \n",
      "1        149190     55629189        Caucasian  Female  [10-20)      ?   \n",
      "2         64410     86047875  AfricanAmerican  Female  [20-30)      ?   \n",
      "3        500364     82442376        Caucasian    Male  [30-40)      ?   \n",
      "4         16680     42519267        Caucasian    Male  [40-50)      ?   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id  \\\n",
      "0                  6                        25                    1   \n",
      "1                  1                         1                    7   \n",
      "2                  1                         1                    7   \n",
      "3                  1                         1                    7   \n",
      "4                  1                         1                    7   \n",
      "\n",
      "   time_in_hospital  ... citoglipton insulin  glyburide-metformin  \\\n",
      "0                 1  ...          No      No                   No   \n",
      "1                 3  ...          No      Up                   No   \n",
      "2                 2  ...          No      No                   No   \n",
      "3                 2  ...          No      Up                   No   \n",
      "4                 1  ...          No  Steady                   No   \n",
      "\n",
      "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone  \\\n",
      "0                   No                        No                       No   \n",
      "1                   No                        No                       No   \n",
      "2                   No                        No                       No   \n",
      "3                   No                        No                       No   \n",
      "4                   No                        No                       No   \n",
      "\n",
      "   metformin-pioglitazone  change diabetesMed readmitted  \n",
      "0                      No      No          No         NO  \n",
      "1                      No      Ch         Yes        >30  \n",
      "2                      No      No         Yes         NO  \n",
      "3                      No      Ch         Yes         NO  \n",
      "4                      No      Ch         Yes         NO  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101766 entries, 0 to 101765\n",
      "Data columns (total 50 columns):\n",
      " #   Column                    Non-Null Count   Dtype \n",
      "---  ------                    --------------   ----- \n",
      " 0   encounter_id              101766 non-null  int64 \n",
      " 1   patient_nbr               101766 non-null  int64 \n",
      " 2   race                      101766 non-null  object\n",
      " 3   gender                    101766 non-null  object\n",
      " 4   age                       101766 non-null  object\n",
      " 5   weight                    101766 non-null  object\n",
      " 6   admission_type_id         101766 non-null  int64 \n",
      " 7   discharge_disposition_id  101766 non-null  int64 \n",
      " 8   admission_source_id       101766 non-null  int64 \n",
      " 9   time_in_hospital          101766 non-null  int64 \n",
      " 10  payer_code                101766 non-null  object\n",
      " 11  medical_specialty         101766 non-null  object\n",
      " 12  num_lab_procedures        101766 non-null  int64 \n",
      " 13  num_procedures            101766 non-null  int64 \n",
      " 14  num_medications           101766 non-null  int64 \n",
      " 15  number_outpatient         101766 non-null  int64 \n",
      " 16  number_emergency          101766 non-null  int64 \n",
      " 17  number_inpatient          101766 non-null  int64 \n",
      " 18  diag_1                    101766 non-null  object\n",
      " 19  diag_2                    101766 non-null  object\n",
      " 20  diag_3                    101766 non-null  object\n",
      " 21  number_diagnoses          101766 non-null  int64 \n",
      " 22  max_glu_serum             101766 non-null  object\n",
      " 23  A1Cresult                 101766 non-null  object\n",
      " 24  metformin                 101766 non-null  object\n",
      " 25  repaglinide               101766 non-null  object\n",
      " 26  nateglinide               101766 non-null  object\n",
      " 27  chlorpropamide            101766 non-null  object\n",
      " 28  glimepiride               101766 non-null  object\n",
      " 29  acetohexamide             101766 non-null  object\n",
      " 30  glipizide                 101766 non-null  object\n",
      " 31  glyburide                 101766 non-null  object\n",
      " 32  tolbutamide               101766 non-null  object\n",
      " 33  pioglitazone              101766 non-null  object\n",
      " 34  rosiglitazone             101766 non-null  object\n",
      " 35  acarbose                  101766 non-null  object\n",
      " 36  miglitol                  101766 non-null  object\n",
      " 37  troglitazone              101766 non-null  object\n",
      " 38  tolazamide                101766 non-null  object\n",
      " 39  examide                   101766 non-null  object\n",
      " 40  citoglipton               101766 non-null  object\n",
      " 41  insulin                   101766 non-null  object\n",
      " 42  glyburide-metformin       101766 non-null  object\n",
      " 43  glipizide-metformin       101766 non-null  object\n",
      " 44  glimepiride-pioglitazone  101766 non-null  object\n",
      " 45  metformin-rosiglitazone   101766 non-null  object\n",
      " 46  metformin-pioglitazone    101766 non-null  object\n",
      " 47  change                    101766 non-null  object\n",
      " 48  diabetesMed               101766 non-null  object\n",
      " 49  readmitted                101766 non-null  object\n",
      "dtypes: int64(13), object(37)\n",
      "memory usage: 38.8+ MB\n",
      "None\n",
      "encounter_id                0\n",
      "patient_nbr                 0\n",
      "race                        0\n",
      "gender                      0\n",
      "age                         0\n",
      "weight                      0\n",
      "admission_type_id           0\n",
      "discharge_disposition_id    0\n",
      "admission_source_id         0\n",
      "time_in_hospital            0\n",
      "payer_code                  0\n",
      "medical_specialty           0\n",
      "num_lab_procedures          0\n",
      "num_procedures              0\n",
      "num_medications             0\n",
      "number_outpatient           0\n",
      "number_emergency            0\n",
      "number_inpatient            0\n",
      "diag_1                      0\n",
      "diag_2                      0\n",
      "diag_3                      0\n",
      "number_diagnoses            0\n",
      "max_glu_serum               0\n",
      "A1Cresult                   0\n",
      "metformin                   0\n",
      "repaglinide                 0\n",
      "nateglinide                 0\n",
      "chlorpropamide              0\n",
      "glimepiride                 0\n",
      "acetohexamide               0\n",
      "glipizide                   0\n",
      "glyburide                   0\n",
      "tolbutamide                 0\n",
      "pioglitazone                0\n",
      "rosiglitazone               0\n",
      "acarbose                    0\n",
      "miglitol                    0\n",
      "troglitazone                0\n",
      "tolazamide                  0\n",
      "examide                     0\n",
      "citoglipton                 0\n",
      "insulin                     0\n",
      "glyburide-metformin         0\n",
      "glipizide-metformin         0\n",
      "glimepiride-pioglitazone    0\n",
      "metformin-rosiglitazone     0\n",
      "metformin-pioglitazone      0\n",
      "change                      0\n",
      "diabetesMed                 0\n",
      "readmitted                  0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV\n",
    "df = pd.read_csv('diabetic_data.csv')\n",
    "\n",
    "# Mostrar las primeras filas del dataset\n",
    "print(df.head())\n",
    "\n",
    "# Resumen de la información del dataset\n",
    "print(df.info())\n",
    "\n",
    "# Mostrar la cantidad de valores nulos por columna\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_12880\\4281049621.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('diabetic_data.csv', na_values='?')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encounter_id                    0\n",
      "patient_nbr                     0\n",
      "race                         2273\n",
      "gender                          0\n",
      "age                             0\n",
      "weight                      98569\n",
      "admission_type_id               0\n",
      "discharge_disposition_id        0\n",
      "admission_source_id             0\n",
      "time_in_hospital                0\n",
      "payer_code                  40256\n",
      "medical_specialty           49949\n",
      "num_lab_procedures              0\n",
      "num_procedures                  0\n",
      "num_medications                 0\n",
      "number_outpatient               0\n",
      "number_emergency                0\n",
      "number_inpatient                0\n",
      "diag_1                         21\n",
      "diag_2                        358\n",
      "diag_3                       1423\n",
      "number_diagnoses                0\n",
      "max_glu_serum                   0\n",
      "A1Cresult                       0\n",
      "metformin                       0\n",
      "repaglinide                     0\n",
      "nateglinide                     0\n",
      "chlorpropamide                  0\n",
      "glimepiride                     0\n",
      "acetohexamide                   0\n",
      "glipizide                       0\n",
      "glyburide                       0\n",
      "tolbutamide                     0\n",
      "pioglitazone                    0\n",
      "rosiglitazone                   0\n",
      "acarbose                        0\n",
      "miglitol                        0\n",
      "troglitazone                    0\n",
      "tolazamide                      0\n",
      "examide                         0\n",
      "citoglipton                     0\n",
      "insulin                         0\n",
      "glyburide-metformin             0\n",
      "glipizide-metformin             0\n",
      "glimepiride-pioglitazone        0\n",
      "metformin-rosiglitazone         0\n",
      "metformin-pioglitazone          0\n",
      "change                          0\n",
      "diabetesMed                     0\n",
      "readmitted                      0\n",
      "dtype: int64\n",
      "El archivo modificado ha sido guardado como 'diabetic_data_modified.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo CSV, tratando '?' como valores nulos\n",
    "df = pd.read_csv('diabetic_data.csv', na_values='?')\n",
    "\n",
    "# Mostrar la cantidad de valores nulos por columna\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Guardar el DataFrame modificado en un nuevo archivo CSV\n",
    "df.to_csv('diabetic_data_modified.csv', index=False)\n",
    "\n",
    "print(\"El archivo modificado ha sido guardado como 'diabetic_data_modified.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_12880\\1560914958.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('diabetic_data_modified.csv')\n",
      "C:\\Users\\grego\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos procesados y guardados en 'processed_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('diabetic_data_modified.csv')\n",
    "\n",
    "# Asegurarse de manejar correctamente los valores nulos\n",
    "# Identificar las columnas con valores nulos según la descripción proporcionada\n",
    "cols_with_missing = ['race', 'weight', 'payer_code', 'medical_specialty', 'diag_1', 'diag_2', 'diag_3']\n",
    "\n",
    "# Imputar los valores nulos con la moda para variables categóricas\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[cols_with_missing] = categorical_imputer.fit_transform(df[cols_with_missing])\n",
    "\n",
    "# Limpiar y convertir 'weight' a valores numéricos y luego categorizarlo\n",
    "def clean_weight(weight_str):\n",
    "    if weight_str == \"?\":\n",
    "        return None\n",
    "    elif weight_str.startswith(\">\"):\n",
    "        return float(weight_str[1:]) + 1  # Incrementar en 1 para asegurar que los límites sean correctos\n",
    "    elif weight_str.startswith(\"[\"):\n",
    "        return float(weight_str.strip(\"[]\").split(\"-\")[0])\n",
    "    elif weight_str == \"Unknown\":\n",
    "        return None\n",
    "    else:\n",
    "        return float(weight_str)\n",
    "\n",
    "df['weight'] = df['weight'].apply(clean_weight)\n",
    "\n",
    "# Definir los rangos de peso y codificar 'weight'\n",
    "weight_ranges = ['[0-25)', '[25-50)', '[50-75)', '[75-100)', '[100-125)', '[125-150)', '[150-175)', '[175-200)', '>200']\n",
    "df['weight_category'] = pd.cut(df['weight'], bins=[0, 25, 50, 75, 100, 125, 150, 175, 200, float('inf')], labels=weight_ranges, right=False)\n",
    "\n",
    "# Eliminar la columna original 'weight'\n",
    "df.drop(columns=['weight'], inplace=True)\n",
    "\n",
    "# Función para asignar categorías a los códigos ICD-9\n",
    "def assign_icd_category(icd_code):\n",
    "    if pd.isnull(icd_code):\n",
    "        return 'Unknown'\n",
    "    if icd_code.startswith(('E', 'V')):\n",
    "        return 'E-V codes'\n",
    "    else:\n",
    "        code_number = int(icd_code.split('.')[0])  # Tomar solo el número de código ICD-9\n",
    "        if 1 <= code_number <= 139:\n",
    "            return '001-139'\n",
    "        elif 140 <= code_number <= 239:\n",
    "            return '140-239'\n",
    "        elif 240 <= code_number <= 279:\n",
    "            return '240-279'\n",
    "        elif 280 <= code_number <= 289:\n",
    "            return '280-289'\n",
    "        elif 290 <= code_number <= 319:\n",
    "            return '290-319'\n",
    "        elif 320 <= code_number <= 389:\n",
    "            return '320-389'\n",
    "        elif 390 <= code_number <= 459:\n",
    "            return '390-459'\n",
    "        elif 460 <= code_number <= 519:\n",
    "            return '460-519'\n",
    "        elif 520 <= code_number <= 579:\n",
    "            return '520-579'\n",
    "        elif 580 <= code_number <= 629:\n",
    "            return '580-629'\n",
    "        elif 630 <= code_number <= 679:\n",
    "            return '630-679'\n",
    "        elif 680 <= code_number <= 709:\n",
    "            return '680-709'\n",
    "        elif 710 <= code_number <= 739:\n",
    "            return '710-739'\n",
    "        elif 740 <= code_number <= 759:\n",
    "            return '740-759'\n",
    "        elif 760 <= code_number <= 779:\n",
    "            return '760-779'\n",
    "        elif 780 <= code_number <= 799:\n",
    "            return '780-799'\n",
    "        elif 800 <= code_number <= 999:\n",
    "            return '800-999'\n",
    "        else:\n",
    "            return 'Other'  # En caso de no encontrar una categoría válida\n",
    "\n",
    "# Aplicar la función a cada columna de diagnóstico\n",
    "for col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    df[col + '_category'] = df[col].apply(assign_icd_category)\n",
    "\n",
    "# Eliminar las columnas originales de diagnóstico\n",
    "df.drop(columns=['diag_1', 'diag_2', 'diag_3'], inplace=True)\n",
    "\n",
    "# Función para asignar valores únicos a las franjas de edad\n",
    "def age_to_value(age_str):\n",
    "    age_mapping = {\n",
    "        '[0-10)': 5,\n",
    "        '[10-20)': 15,\n",
    "        '[20-30)': 25,\n",
    "        '[30-40)': 35,\n",
    "        '[40-50)': 45,\n",
    "        '[50-60)': 55,\n",
    "        '[60-70)': 65,\n",
    "        '[70-80)': 75,\n",
    "        '[80-90)': 85,\n",
    "        '[90-100)': 95\n",
    "    }\n",
    "    return age_mapping.get(age_str, None)\n",
    "\n",
    "# Aplicar la función de agrupamiento de edades\n",
    "df['age'] = df['age'].apply(age_to_value)\n",
    "\n",
    "# Aplicar codificación one-hot a las variables categóricas, excluyendo 'age' ya que está mapeada a valores únicos\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'age' in categorical_cols:\n",
    "    categorical_cols.remove('age')\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(df[categorical_cols]))\n",
    "\n",
    "# Sustituir las columnas originales con las nuevas codificadas\n",
    "encoded_cols.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "df.drop(columns=categorical_cols, inplace=True)\n",
    "df = pd.concat([df, encoded_cols], axis=1)\n",
    "\n",
    "# Guardar el resultado en un nuevo archivo CSV\n",
    "df.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "print(\"Datos procesados y guardados en 'processed_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo de red neuronal, obtención de bias, pesos y métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos y sesgos antes del entrenamiento:\n",
      "Layer 0 weights:\n",
      "[[-0.03503197 -0.07515648 -0.00513428 ... -0.06696251 -0.03175566\n",
      "   0.03766366]\n",
      " [-0.00125447  0.05520108  0.03046366 ...  0.0960763  -0.05543126\n",
      "   0.06987789]\n",
      " [-0.07501873 -0.09705907 -0.00146549 ...  0.0348407  -0.04024605\n",
      "  -0.06422001]\n",
      " ...\n",
      " [ 0.0397206   0.10499511  0.09925058 ... -0.09617956 -0.05967144\n",
      "   0.04502701]\n",
      " [ 0.08002459 -0.06073426  0.05849119 ... -0.01215655  0.00570613\n",
      "  -0.00833569]\n",
      " [ 0.08311635 -0.02588409 -0.09078434 ...  0.06704467  0.07918165\n",
      "   0.01881223]]\n",
      "\n",
      "Layer 0 biases:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Layer 1 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 2 weights:\n",
      "[[ 0.02198447 -0.08893353 -0.00086755 ... -0.08381981  0.10665882\n",
      "   0.11673587]\n",
      " [ 0.04664206  0.09107334 -0.06988677 ...  0.03035813  0.06127404\n",
      "   0.04266419]\n",
      " [-0.12193373 -0.00958584  0.0546725  ...  0.09059551 -0.00057619\n",
      "  -0.03364729]\n",
      " ...\n",
      " [ 0.11082303  0.12268373 -0.11299664 ... -0.07706067 -0.06230036\n",
      "   0.07139552]\n",
      " [-0.01424191 -0.04012734 -0.10055758 ...  0.11223496  0.0485707\n",
      "  -0.11412283]\n",
      " [ 0.01396065  0.11656881 -0.01652256 ...  0.09783507 -0.0734912\n",
      "   0.1169159 ]]\n",
      "\n",
      "Layer 2 biases:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Layer 3 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 4 weights:\n",
      "[[-0.13763824 -0.1105371  -0.0483252  ...  0.05015159 -0.08299515\n",
      "  -0.04767516]\n",
      " [-0.12108064 -0.03801557 -0.06322883 ...  0.16099606  0.02662745\n",
      "  -0.09321673]\n",
      " [-0.01595971  0.17651255  0.059126   ...  0.05382548 -0.11375973\n",
      "   0.13566153]\n",
      " ...\n",
      " [ 0.11511062  0.02507728 -0.03355078 ... -0.14380467 -0.13322808\n",
      "  -0.07933491]\n",
      " [-0.11191092 -0.032249    0.14001958 ...  0.05795795 -0.07897708\n",
      "  -0.0478586 ]\n",
      " [-0.14160237  0.11833559  0.0768394  ... -0.03213456  0.07860462\n",
      "  -0.14183198]]\n",
      "\n",
      "Layer 4 biases:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Layer 5 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 6 weights:\n",
      "[[ 0.14253795 -0.01917219 -0.1050427  ...  0.09117007  0.16612828\n",
      "   0.18427372]\n",
      " [ 0.05307728 -0.03203952 -0.23736739 ... -0.10497761  0.22843194\n",
      "   0.1390993 ]\n",
      " [ 0.06518954 -0.16874254 -0.0936749  ... -0.00279993 -0.21741927\n",
      "   0.1792162 ]\n",
      " ...\n",
      " [ 0.17063308 -0.06592613  0.00820982 ...  0.00517642  0.12456322\n",
      "  -0.19046545]\n",
      " [-0.23401427 -0.06817168  0.15684032 ... -0.13085705  0.03473473\n",
      "  -0.12639648]\n",
      " [-0.17604959 -0.01638585 -0.02287972 ... -0.24860424  0.0100444\n",
      "   0.1975379 ]]\n",
      "\n",
      "Layer 6 biases:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Layer 7 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 8 weights:\n",
      "[[-0.32013768]\n",
      " [ 0.1932717 ]\n",
      " [-0.33044752]\n",
      " [-0.41780806]\n",
      " [-0.1463745 ]\n",
      " [-0.19357283]\n",
      " [ 0.05861008]\n",
      " [-0.18916486]\n",
      " [-0.3620761 ]\n",
      " [-0.22498538]\n",
      " [ 0.07287022]\n",
      " [ 0.09399956]\n",
      " [ 0.35576332]\n",
      " [-0.34903765]\n",
      " [-0.01831961]\n",
      " [ 0.28910547]\n",
      " [-0.26569715]\n",
      " [-0.25912878]\n",
      " [-0.1907201 ]\n",
      " [-0.0326632 ]\n",
      " [-0.31007272]\n",
      " [ 0.11312026]\n",
      " [ 0.35120195]\n",
      " [ 0.13142401]\n",
      " [ 0.09364301]\n",
      " [-0.20156428]\n",
      " [-0.25316253]\n",
      " [ 0.36904675]\n",
      " [-0.11821991]\n",
      " [-0.23517242]\n",
      " [-0.28364792]\n",
      " [ 0.02274048]]\n",
      "\n",
      "Layer 8 biases:\n",
      "[0.]\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8763 - loss: 0.0824 - val_accuracy: 0.9958 - val_loss: 0.0041\n",
      "Epoch 2/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9956 - loss: 0.0040 - val_accuracy: 0.9983 - val_loss: 0.0016\n",
      "Epoch 3/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9962 - loss: 0.0037 - val_accuracy: 0.9978 - val_loss: 0.0022\n",
      "Epoch 4/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9967 - loss: 0.0030 - val_accuracy: 0.9980 - val_loss: 0.0020\n",
      "Epoch 5/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9973 - loss: 0.0026 - val_accuracy: 0.9978 - val_loss: 0.0019\n",
      "Epoch 6/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9970 - loss: 0.0029 - val_accuracy: 0.9983 - val_loss: 0.0015\n",
      "Epoch 7/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9983 - loss: 0.0015 - val_accuracy: 0.9995 - val_loss: 4.9128e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9986 - loss: 0.0013 - val_accuracy: 0.9985 - val_loss: 0.0014\n",
      "Epoch 9/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9980 - loss: 0.0019 - val_accuracy: 0.9990 - val_loss: 9.8411e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m573/573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.9978 - loss: 0.0022 - val_accuracy: 0.9980 - val_loss: 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precisión durante el entrenamiento:\n",
      "Precisión en el conjunto de entrenamiento:\n",
      "[95.56162357330322, 99.54960942268372, 99.61512088775635, 99.68063235282898, 99.7215747833252, 99.7215747833252, 99.84986782073975, 99.85805749893188, 99.84168410301208, 99.74887371063232]\n",
      "\n",
      "Precisión en el conjunto de validación:\n",
      "[99.58240985870361, 99.82805252075195, 99.77892637252808, 99.80348944664001, 99.77892637252808, 99.82805252075195, 99.95087385177612, 99.85261559486389, 99.90174174308777, 99.80348944664001]\n",
      "\n",
      "Accuracy promedio en entrenamiento: 99.31%\n",
      "Accuracy promedio en validación: 99.81%\n",
      "\n",
      "Pesos y sesgos después del entrenamiento:\n",
      "Layer 0 weights:\n",
      "[[ 1.98230948e-02 -5.59530444e-02  5.91089651e-02 ... -1.01457917e-05\n",
      "   5.31534962e-02  1.08917661e-01]\n",
      " [ 5.92324473e-02  1.00996986e-01  1.09652855e-01 ...  6.75360188e-02\n",
      "  -7.30945095e-02  1.46710560e-01]\n",
      " [-6.80574626e-02 -6.30808547e-02  3.97472316e-03 ...  4.97060120e-02\n",
      "   7.91861713e-02 -1.87372327e-01]\n",
      " ...\n",
      " [ 6.91070557e-02  8.07609782e-02  3.73063497e-02 ... -9.40403566e-02\n",
      "  -2.36536004e-02 -1.54953720e-02]\n",
      " [ 4.31819893e-02 -1.08925529e-01  1.14034437e-01 ... -4.59038243e-02\n",
      "  -3.18355821e-02  4.14865837e-02]\n",
      " [ 1.35181084e-01 -2.90132780e-02  9.68736410e-03 ...  1.31013719e-02\n",
      "   7.22142830e-02 -4.14418280e-02]]\n",
      "\n",
      "Layer 0 biases:\n",
      "[ 4.85490412e-02  7.51314536e-02 -1.08605877e-01  1.76865589e-02\n",
      " -1.27895787e-01 -4.66522351e-02 -1.54730575e-02  9.48464647e-02\n",
      "  1.19562827e-01  8.30102414e-02 -1.13065265e-01 -9.17536020e-02\n",
      "  1.57436095e-02  9.86543223e-02  9.33923647e-02  8.13186392e-02\n",
      " -4.71844785e-02 -4.43451144e-02 -2.87649967e-02 -1.40124366e-01\n",
      " -1.25131845e-01 -4.61253785e-02  2.47139372e-02 -1.35650471e-01\n",
      "  9.50857550e-02  9.24142972e-02  7.98219368e-02 -7.04314467e-03\n",
      "  1.01597711e-01  2.27863304e-02 -1.22650973e-01 -7.94995055e-02\n",
      " -9.12960917e-02  6.64475113e-02  6.99876696e-02  2.82016098e-02\n",
      " -1.02003038e-01  3.82400714e-02  7.45858625e-02 -1.12338409e-01\n",
      " -1.06306866e-01  2.57964190e-02 -2.57071294e-02 -5.44993617e-02\n",
      " -9.28918943e-02 -4.90832143e-02  6.27118349e-02  1.14326246e-01\n",
      "  9.60459933e-02  5.55850044e-02 -2.30983011e-02  1.44658089e-01\n",
      " -9.73000303e-02  2.03453191e-02 -5.50156273e-03  3.59304473e-02\n",
      "  4.60381694e-02  7.01957941e-02  1.23213179e-01  8.32975730e-02\n",
      "  1.23344950e-01  1.81251898e-01  7.15645477e-02 -2.83689648e-02\n",
      " -9.89725068e-02  7.54173025e-02 -7.96963423e-02  6.66655228e-02\n",
      "  7.26490319e-02 -1.39568180e-01 -1.11219496e-01 -1.17725424e-01\n",
      "  1.12964679e-02  9.73414071e-03  1.81343704e-01  6.41908124e-02\n",
      " -1.19030491e-01  3.51448841e-02 -1.05469316e-01  8.92625526e-02\n",
      "  9.95275155e-02  4.55198064e-02  3.74946669e-02  5.89145087e-02\n",
      " -1.11364618e-01  4.33964320e-02 -5.92138208e-02  1.51130483e-01\n",
      " -2.36816630e-02  9.35635567e-02  3.65387648e-02  2.58925613e-02\n",
      "  4.14421894e-02 -8.97297114e-02 -9.16265994e-02  4.60046306e-02\n",
      " -1.21234814e-02 -2.43373588e-02  1.46647081e-01  8.28849077e-02\n",
      " -1.06378138e-01  8.95925909e-02 -5.36205573e-03 -1.29841983e-01\n",
      "  9.08598453e-02  8.36768299e-02  2.37536356e-02  5.26750870e-02\n",
      "  7.56449550e-02  5.63418083e-02  1.20275401e-01  2.56555397e-02\n",
      "  1.49710864e-01  4.24440689e-02  4.58910083e-03 -1.32565275e-01\n",
      "  4.52502593e-02 -8.80704299e-02 -1.27994478e-01 -1.00954674e-01\n",
      "  5.79556562e-02  2.89042480e-02 -6.71494901e-02  3.37292477e-02\n",
      "  5.53670749e-02  6.70001954e-02 -1.44530058e-01  4.47757356e-02\n",
      "  1.06975317e-01  7.25100562e-02 -5.02227992e-02  1.20851370e-02\n",
      " -3.13232467e-02  6.03533499e-02 -3.84910107e-02  2.79994775e-02\n",
      " -8.98129120e-02 -1.23028969e-02 -1.61974832e-01 -3.22303064e-02\n",
      " -8.75537395e-02  1.73716411e-01  1.18651368e-01  2.81710681e-02\n",
      " -1.89396329e-02 -1.22802764e-01 -7.38369599e-02  3.08541693e-02\n",
      " -1.66689437e-02  7.48937428e-02 -1.30712584e-01  2.01971531e-02\n",
      "  7.90243596e-02  2.77776476e-02  1.05210515e-02  8.68835673e-02\n",
      "  1.88482238e-03 -1.71750009e-01 -6.38361052e-02  6.72052056e-02\n",
      "  1.06660306e-01 -7.37962797e-02  5.21005690e-03 -7.29961172e-02\n",
      "  1.24058791e-01 -8.51038918e-02  7.39335567e-02 -5.97908013e-02\n",
      " -5.39977178e-02 -1.63924754e-01  1.06925368e-01  3.70681956e-02\n",
      "  1.02511920e-01 -3.24514024e-02 -3.23635712e-02  1.37794986e-01\n",
      " -1.12925567e-01 -9.03681889e-02  1.10794425e-01  5.29320091e-02\n",
      "  9.46575254e-02 -3.23402695e-02  1.28127709e-01  6.31632730e-02\n",
      "  8.94116536e-02 -8.81623775e-02  1.92962699e-02  7.33312368e-02\n",
      "  1.69693530e-02 -2.47758608e-02  1.74362555e-01 -5.26328513e-05\n",
      "  6.61713108e-02  5.03564179e-02  3.66124734e-02  4.03132625e-02\n",
      " -4.70463075e-02 -4.48453724e-02  9.98754576e-02  1.02047101e-01\n",
      "  7.75551349e-02 -1.13066845e-02 -1.07759945e-02 -8.24377015e-02\n",
      " -1.28282607e-01 -4.79497463e-02 -1.09536655e-01  4.12727594e-02\n",
      "  7.32186362e-02 -7.33619854e-02  2.65344549e-02 -1.00176699e-01\n",
      "  6.97524622e-02  1.61818355e-01  8.01025182e-02 -1.00090437e-01\n",
      " -4.48333286e-02 -6.39240593e-02 -1.26343355e-01 -5.77787794e-02\n",
      "  8.29441249e-02 -6.95078000e-02 -9.00111219e-04  9.90015268e-02\n",
      " -9.11156461e-02  1.04580432e-01 -4.15820181e-02 -1.26843318e-01\n",
      "  2.68393643e-02  5.92792407e-02  3.98907028e-02  1.11852720e-01\n",
      " -1.63460746e-01 -2.29549650e-02 -1.73710763e-01  6.16535097e-02\n",
      " -7.38477930e-02 -9.12268534e-02 -1.17263019e-01 -1.23058490e-01\n",
      " -1.06105678e-01 -5.19843474e-02 -3.68424617e-02  1.56856388e-01\n",
      " -1.14586145e-01 -4.31882031e-02 -7.38955289e-02  7.85403326e-03\n",
      " -6.70881495e-02  5.90796731e-02 -3.15259770e-02  6.82400316e-02\n",
      "  6.61152229e-02 -1.08671807e-01  9.50582102e-02 -6.24557622e-02\n",
      "  6.80316687e-02 -7.00564161e-02]\n",
      "\n",
      "Layer 1 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 2 weights:\n",
      "[[ 0.07695702 -0.10665663  0.02955161 ... -0.11135057  0.10128617\n",
      "   0.11184809]\n",
      " [ 0.05284599  0.06856173 -0.07189447 ...  0.09227491  0.06692402\n",
      "   0.03381958]\n",
      " [-0.10043558 -0.07325992  0.09373685 ...  0.20728217 -0.01971457\n",
      "  -0.07514324]\n",
      " ...\n",
      " [ 0.18995027  0.18496802 -0.04529789 ... -0.10596898 -0.07531069\n",
      "   0.0550459 ]\n",
      " [ 0.11414321 -0.10571784  0.00618992 ...  0.22481962  0.01676712\n",
      "  -0.09569393]\n",
      " [ 0.01936743  0.05776901 -0.00477746 ...  0.12860897 -0.08836065\n",
      "   0.11747592]]\n",
      "\n",
      "Layer 2 biases:\n",
      "[ 0.02653638 -0.04791654  0.03233113 -0.04706392  0.05346857 -0.01525649\n",
      " -0.04386581  0.03446314 -0.03779272 -0.01746739 -0.02869433 -0.02353375\n",
      " -0.00525375 -0.04168921 -0.00174724 -0.035375   -0.00935271 -0.02666083\n",
      " -0.05400768  0.04388285 -0.02907693  0.04633145 -0.01535663  0.00906272\n",
      " -0.00641792 -0.05741136  0.00855091  0.01309706 -0.00595151  0.08806241\n",
      "  0.01327246  0.05529743 -0.03039972 -0.01170897  0.03505898  0.09252784\n",
      " -0.0317267  -0.00834325 -0.01344278 -0.02894008 -0.03786401 -0.03461256\n",
      "  0.06619298  0.08461228 -0.01861722 -0.02568526 -0.02610963  0.04611158\n",
      " -0.02078589  0.0048092  -0.02024292  0.01043798 -0.01519696  0.02343994\n",
      "  0.0033618   0.06562319 -0.01103448 -0.03933956  0.04705224 -0.01750235\n",
      "  0.04658714 -0.00672333 -0.04463639  0.05053843 -0.00437506 -0.02863788\n",
      "  0.03811592  0.04444975 -0.03008808  0.01154295 -0.01754317  0.04904816\n",
      "  0.00700706 -0.02188225  0.00927207  0.00333184  0.03482012  0.00026973\n",
      "  0.08894724  0.0529808   0.035526   -0.01777476  0.09373279  0.00890901\n",
      " -0.01971109 -0.03168012  0.00511717  0.04974417  0.10701284 -0.01303026\n",
      "  0.02136769  0.02060984  0.02868948 -0.02483608 -0.00400871 -0.02287249\n",
      " -0.05227102 -0.02346024 -0.0343253  -0.05223774 -0.0576146  -0.03855819\n",
      " -0.03530012 -0.06147332 -0.00569229 -0.01136383 -0.02278852 -0.02955951\n",
      " -0.02164558  0.02382283 -0.01968792  0.07593758 -0.02655152 -0.0280614\n",
      "  0.03060526 -0.02732145 -0.0191584  -0.02642331 -0.03939658  0.06900435\n",
      " -0.03638746 -0.00757615 -0.03869007  0.01942769 -0.03013128  0.05476202\n",
      " -0.026459   -0.01418639]\n",
      "\n",
      "Layer 3 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 4 weights:\n",
      "[[-0.1684954  -0.17697382 -0.06962843 ... -0.03877646 -0.09406848\n",
      "  -0.01566773]\n",
      " [-0.17227502  0.0642131  -0.07552579 ...  0.16439632 -0.00071673\n",
      "   0.01096069]\n",
      " [-0.03012595  0.2006495   0.02692054 ...  0.07013524 -0.14675143\n",
      "   0.10911202]\n",
      " ...\n",
      " [ 0.0685763   0.0723213  -0.06117744 ... -0.2064103  -0.2080415\n",
      "  -0.03958326]\n",
      " [-0.12325149 -0.0334051   0.13804282 ...  0.05304887 -0.10278749\n",
      "  -0.01717446]\n",
      " [-0.15204357  0.12035728  0.05566082 ... -0.06719343  0.06060243\n",
      "  -0.08433063]]\n",
      "\n",
      "Layer 4 biases:\n",
      "[-0.02544289 -0.01121957 -0.04068373  0.0921903  -0.02917154  0.02071473\n",
      "  0.00888518  0.00045209  0.00366581 -0.02806702 -0.03217911 -0.02073618\n",
      "  0.04644239  0.03231883 -0.04702472 -0.00902428  0.05382533  0.04764168\n",
      "  0.00843218 -0.02407924  0.0317074  -0.0268232   0.01435009  0.00953124\n",
      " -0.04634777  0.0097484   0.00843017 -0.00058932 -0.04052026  0.04338335\n",
      "  0.03987052 -0.02985521 -0.0262749  -0.05704854 -0.01007696 -0.01557086\n",
      " -0.01202857  0.02773137 -0.02498944  0.05842734 -0.03193646 -0.05100036\n",
      " -0.02548427 -0.00946914 -0.04208939  0.03835518  0.01833195 -0.01679488\n",
      "  0.00563701 -0.06527962 -0.01221372  0.02051108  0.02801009  0.00384235\n",
      " -0.02977455  0.06276299  0.01770247  0.00763269  0.02601742  0.02906567\n",
      "  0.08130067 -0.03683319  0.01657408  0.06324589]\n",
      "\n",
      "Layer 5 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 6 weights:\n",
      "[[ 0.15614653 -0.04002148 -0.1082789  ...  0.09799016  0.18097727\n",
      "   0.1669665 ]\n",
      " [ 0.06169512 -0.00195828 -0.18547834 ... -0.18313193  0.21301962\n",
      "   0.12824671]\n",
      " [ 0.05027747 -0.14327563 -0.10476503 ... -0.0187451  -0.1941158\n",
      "   0.24269637]\n",
      " ...\n",
      " [ 0.18793043 -0.09501426  0.0376786  ...  0.01734844  0.14495617\n",
      "  -0.20907442]\n",
      " [-0.18543956 -0.11822774  0.20113815 ... -0.09437276  0.06364376\n",
      "  -0.19034612]\n",
      " [-0.07281923 -0.03490973  0.06630979 ... -0.20757765  0.03906955\n",
      "   0.18311693]]\n",
      "\n",
      "Layer 6 biases:\n",
      "[ 0.01946509  0.01702107  0.0090208  -0.01188891 -0.01223322 -0.0352918\n",
      "  0.06194307 -0.02059267  0.01408934  0.03758974  0.02060497  0.03221034\n",
      " -0.00597224  0.028126   -0.02770518 -0.01678981 -0.00770121  0.02761649\n",
      " -0.0204234   0.01945942  0.10356927  0.0363642   0.01530258  0.03737772\n",
      "  0.02024194 -0.00732808 -0.01622624  0.07060415 -0.04298797 -0.03304631\n",
      " -0.01720015  0.03965521]\n",
      "\n",
      "Layer 7 (Dropout) no tiene pesos ni bias.\n",
      "\n",
      "Layer 8 weights:\n",
      "[[-0.28632373]\n",
      " [ 0.20752208]\n",
      " [-0.3192588 ]\n",
      " [-0.4087684 ]\n",
      " [-0.10821331]\n",
      " [-0.19150612]\n",
      " [ 0.17222045]\n",
      " [-0.12555203]\n",
      " [-0.34195024]\n",
      " [-0.21678421]\n",
      " [ 0.1316543 ]\n",
      " [ 0.18998034]\n",
      " [ 0.3688397 ]\n",
      " [-0.35181546]\n",
      " [ 0.01888542]\n",
      " [ 0.32069528]\n",
      " [-0.2577667 ]\n",
      " [-0.2664324 ]\n",
      " [-0.11957454]\n",
      " [-0.07116251]\n",
      " [-0.37186685]\n",
      " [ 0.20047326]\n",
      " [ 0.35736778]\n",
      " [ 0.21621756]\n",
      " [ 0.15944564]\n",
      " [-0.21438389]\n",
      " [-0.19641653]\n",
      " [ 0.44898316]\n",
      " [-0.0496767 ]\n",
      " [-0.17047447]\n",
      " [-0.27521944]\n",
      " [ 0.10802185]]\n",
      "\n",
      "Layer 8 biases:\n",
      "[0.01576778]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9969 - loss: 0.0028\n",
      "Loss en el conjunto de prueba: 0.0026125183794647455\n",
      "Accuracy en el conjunto de prueba: 0.9973469376564026\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Matriz de Confusión:\n",
      "[[2087   15]\n",
      " [  12 8063]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      2102\n",
      "         1.0       1.00      1.00      1.00      8075\n",
      "\n",
      "    accuracy                           1.00     10177\n",
      "   macro avg       1.00      1.00      1.00     10177\n",
      "weighted avg       1.00      1.00      1.00     10177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('processed_data.csv')\n",
    "X = df.drop(columns=['diabetesMed_Yes']) \n",
    "Y = df['diabetesMed_Yes']\n",
    "\n",
    "\n",
    "X_subset = X.iloc[50883: , :]\n",
    "y_subset = Y.iloc[50883: ]\n",
    "\n",
    "# Dividir en conjunto de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subset,y_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar los datos de entrada\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = Sequential()\n",
    "\n",
    "# Añadir capas densas\n",
    "model.add(Dense(258, activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))  \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(32, activation='relu'))  \n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilar el modelo\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Función para obtener y mostrar pesos y sesgos\n",
    "def print_layer_weights(model):\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        if len(weights) > 0: \n",
    "            weights, biases = weights\n",
    "            print(f\"Layer {i} weights:\\n{weights}\\n\")\n",
    "            print(f\"Layer {i} biases:\\n{biases}\\n\")\n",
    "        else:\n",
    "            print(f\"Layer {i} ({layer.__class__.__name__}) no tiene pesos ni bias.\\n\")\n",
    "\n",
    "\n",
    "# Mostrar pesos y sesgos antes del entrenamiento\n",
    "print(\"Pesos y sesgos antes del entrenamiento:\")\n",
    "print_layer_weights(model)\n",
    "\n",
    "# Entrenar el modelo\n",
    "parametrosModelo = model.fit(X_train_scaled, y_train, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Mostrar precisión después del entrenamiento\n",
    "print()\n",
    "print(\"Precisión durante el entrenamiento:\")\n",
    "print(\"Precisión en el conjunto de entrenamiento:\")\n",
    "print([acc * 100 for acc in parametrosModelo.history['accuracy']])\n",
    "print()\n",
    "print(\"Precisión en el conjunto de validación:\")\n",
    "print([acc * 100 for acc in parametrosModelo.history['val_accuracy']]) \n",
    "\n",
    "# Calcular accuracy promedio\n",
    "avg_accuracy_train = np.mean(parametrosModelo.history['accuracy']) * 100\n",
    "avg_accuracy_val = np.mean(parametrosModelo.history['val_accuracy']) * 100\n",
    "\n",
    "print()\n",
    "print(f\"Accuracy promedio en entrenamiento: {avg_accuracy_train:.2f}%\")\n",
    "print(f\"Accuracy promedio en validación: {avg_accuracy_val:.2f}%\")\n",
    "\n",
    "# Mostrar pesos y sesgos después del entrenamiento\n",
    "print()\n",
    "print(\"Pesos y sesgos después del entrenamiento:\")\n",
    "print_layer_weights(model)\n",
    "\n",
    "\n",
    "model.save('diabetes_model2.h5')\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}\")\n",
    "print(f\"Accuracy en el conjunto de prueba: {test_accuracy}\")\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluar métricas adicionales\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, predicted_classes))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, predicted_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9969 - loss: 0.0028\n",
      "Loss en el conjunto de prueba: 0.0026125183794647455\n",
      "Accuracy en el conjunto de prueba: 0.9973469376564026\n",
      "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Matriz de Confusión:\n",
      "[[2087   15]\n",
      " [  12 8063]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      2102\n",
      "         1.0       1.00      1.00      1.00      8075\n",
      "\n",
      "    accuracy                           1.00     10177\n",
      "   macro avg       1.00      1.00      1.00     10177\n",
      "weighted avg       1.00      1.00      1.00     10177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Loss en el conjunto de prueba: {test_loss}\")\n",
    "print(f\"Accuracy en el conjunto de prueba: {test_accuracy}\")\n",
    "\n",
    "# Hacer predicciones\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluar métricas adicionales\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, predicted_classes))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, predicted_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de los datos en 2 modelos y 1 test y creación de los mismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.9226 - loss: 0.1933 - val_accuracy: 0.9985 - val_loss: 0.0068\n",
      "Epoch 2/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9973 - loss: 0.0091 - val_accuracy: 0.9971 - val_loss: 0.0067\n",
      "Epoch 3/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9976 - loss: 0.0091 - val_accuracy: 0.9974 - val_loss: 0.0089\n",
      "Epoch 4/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 0.9988 - val_loss: 0.0080\n",
      "Epoch 5/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9984 - loss: 0.0043 - val_accuracy: 0.9977 - val_loss: 0.0057\n",
      "Epoch 6/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0025 - val_accuracy: 0.9968 - val_loss: 0.0063\n",
      "Epoch 7/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.9989 - loss: 0.0041 - val_accuracy: 0.9959 - val_loss: 0.0075\n",
      "Epoch 8/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 0.9961 - val_loss: 0.0063\n",
      "Epoch 9/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9990 - loss: 0.0028 - val_accuracy: 0.9983 - val_loss: 0.0033\n",
      "Epoch 10/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0033 - val_accuracy: 0.9962 - val_loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9096 - loss: 0.2046 - val_accuracy: 0.9993 - val_loss: 0.0045\n",
      "Epoch 2/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9977 - loss: 0.0112 - val_accuracy: 0.9983 - val_loss: 0.0048\n",
      "Epoch 3/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 0.9990 - val_loss: 0.0018\n",
      "Epoch 4/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0043 - val_accuracy: 0.9995 - val_loss: 0.0023\n",
      "Epoch 5/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9987 - loss: 0.0047 - val_accuracy: 0.9990 - val_loss: 0.0022\n",
      "Epoch 6/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0025 - val_accuracy: 0.9991 - val_loss: 0.0029\n",
      "Epoch 7/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.9992 - loss: 0.0052 - val_accuracy: 0.9991 - val_loss: 0.0014\n",
      "Epoch 8/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9994 - loss: 0.0032 - val_accuracy: 0.9990 - val_loss: 0.0027\n",
      "Epoch 9/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 0.9990 - val_loss: 0.0058\n",
      "Epoch 10/10\n",
      "\u001b[1m1018/1018\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9995 - val_loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "\n",
    "# Separar características y variable objetivo\n",
    "X = df.drop(columns=['diabetesMed_Yes'])\n",
    "Y = df['diabetesMed_Yes']\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Separar los datos para los dos modelos\n",
    "X_train1 = X_scaled[:40706, :]\n",
    "y_train1 = Y.iloc[:40706]\n",
    "\n",
    "X_train2 = X_scaled[40707:81412, :]\n",
    "y_train2 = Y.iloc[40707:81412]\n",
    "\n",
    "# Datos para prueba\n",
    "X_test = X_scaled[81412:, :]\n",
    "y_test = Y.iloc[81412:]\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(258, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Crear y compilar el modelo para el primer conjunto de datos\n",
    "model1 = create_model(X_train1.shape[1])\n",
    "model1.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model1.fit(X_train1, y_train1, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Guardar el modelo 1\n",
    "model1.save('model1.h5')\n",
    "\n",
    "# Crear y compilar el modelo para el segundo conjunto de datos\n",
    "model2 = create_model(X_train2.shape[1])\n",
    "model2.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(X_train2, y_train2, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Guardar el modelo 2\n",
    "model2.save('model2.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de predicciones y su media mediante bagging y evaluarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "\u001b[1m637/637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "Confusion Matrix:\n",
      "[[ 4176     3]\n",
      " [   12 16163]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      4179\n",
      "         1.0       1.00      1.00      1.00     16175\n",
      "\n",
      "    accuracy                           1.00     20354\n",
      "   macro avg       1.00      1.00      1.00     20354\n",
      "weighted avg       1.00      1.00      1.00     20354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar los modelos entrenados\n",
    "model1 = load_model('model1.h5')\n",
    "model2 = load_model('model2.h5')\n",
    "\n",
    "# Función para hacer predicciones con Bagging\n",
    "def bagging_predict(models, data):\n",
    "    predictions = [model.predict(data) for model in models]\n",
    "    avg_prediction = np.mean(predictions, axis=0)\n",
    "    return avg_prediction\n",
    "\n",
    "# Hacer predicciones con Bagging en los datos de prueba\n",
    "models = [model1, model2]\n",
    "y_pred_bagging = bagging_predict(models, X_test)\n",
    "y_pred_bagging = (y_pred_bagging > 0.5).astype(int)\n",
    "\n",
    "# Evaluar el rendimiento del modelo combinado\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_bagging)\n",
    "class_report = classification_report(y_test, y_pred_bagging)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de los datos en 5 modelos y 1 test y creación de los mismos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 0\n",
      "Epoch 1/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.8572 - loss: 0.3295 - val_accuracy: 0.9965 - val_loss: 0.0263\n",
      "Epoch 2/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9940 - loss: 0.0244 - val_accuracy: 0.9979 - val_loss: 0.0051\n",
      "Epoch 3/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9979 - loss: 0.0137 - val_accuracy: 0.9973 - val_loss: 0.0060\n",
      "Epoch 4/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9984 - loss: 0.0110 - val_accuracy: 0.9968 - val_loss: 0.0096\n",
      "Epoch 5/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9979 - loss: 0.0073 - val_accuracy: 0.9973 - val_loss: 0.0073\n",
      "Epoch 6/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0037 - val_accuracy: 0.9973 - val_loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 1\n",
      "Epoch 1/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.8485 - loss: 0.3214 - val_accuracy: 0.9973 - val_loss: 0.0271\n",
      "Epoch 2/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9940 - loss: 0.0371 - val_accuracy: 0.9982 - val_loss: 0.0098\n",
      "Epoch 3/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9975 - loss: 0.0120 - val_accuracy: 0.9976 - val_loss: 0.0122\n",
      "Epoch 4/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9982 - loss: 0.0121 - val_accuracy: 0.9985 - val_loss: 0.0081\n",
      "Epoch 5/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9986 - loss: 0.0114 - val_accuracy: 0.9982 - val_loss: 0.0090\n",
      "Epoch 6/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9990 - loss: 0.0056 - val_accuracy: 0.9976 - val_loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 2\n",
      "Epoch 1/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8557 - loss: 0.2982 - val_accuracy: 0.9962 - val_loss: 0.0197\n",
      "Epoch 2/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0229 - val_accuracy: 0.9991 - val_loss: 0.0104\n",
      "Epoch 3/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9974 - loss: 0.0154 - val_accuracy: 0.9985 - val_loss: 0.0182\n",
      "Epoch 4/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9982 - loss: 0.0080 - val_accuracy: 0.9988 - val_loss: 0.0125\n",
      "Epoch 5/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9978 - loss: 0.0096 - val_accuracy: 0.9979 - val_loss: 0.0238\n",
      "Epoch 6/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9980 - loss: 0.0072 - val_accuracy: 0.9965 - val_loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 3\n",
      "Epoch 1/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.8607 - loss: 0.3148 - val_accuracy: 0.9962 - val_loss: 0.0290\n",
      "Epoch 2/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0303 - val_accuracy: 0.9985 - val_loss: 0.0139\n",
      "Epoch 3/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9968 - loss: 0.0156 - val_accuracy: 0.9982 - val_loss: 0.0044\n",
      "Epoch 4/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9988 - loss: 0.0087 - val_accuracy: 0.9985 - val_loss: 0.0045\n",
      "Epoch 5/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9975 - loss: 0.0088 - val_accuracy: 0.9988 - val_loss: 0.0096\n",
      "Epoch 6/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9986 - loss: 0.0088 - val_accuracy: 0.9982 - val_loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo 4\n",
      "Epoch 1/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.8612 - loss: 0.2983 - val_accuracy: 0.9971 - val_loss: 0.0191\n",
      "Epoch 2/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9943 - loss: 0.0312 - val_accuracy: 0.9985 - val_loss: 0.0128\n",
      "Epoch 3/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9970 - loss: 0.0128 - val_accuracy: 0.9991 - val_loss: 0.0064\n",
      "Epoch 4/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 0.9979 - val_loss: 0.0125\n",
      "Epoch 5/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9990 - loss: 0.0035 - val_accuracy: 0.9991 - val_loss: 0.0108\n",
      "Epoch 6/6\n",
      "\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9993 - loss: 0.0020 - val_accuracy: 0.9973 - val_loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('processed_data.csv')\n",
    "\n",
    "# Separar características y variable objetivo\n",
    "X = df.drop(columns=['diabetesMed_Yes'])\n",
    "Y = df['diabetesMed_Yes']\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en 2 conjuntos de entrenamiento de 16961 filas cada uno\n",
    "train_size = 40706\n",
    "num_models = 2\n",
    "\n",
    "# Dividir los datos en 5 conjuntos de entrenamiento de 16961 filas cada uno\n",
    "train_size = 16961\n",
    "num_models = 5\n",
    "\n",
    "X_trains = []\n",
    "y_trains = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    start_index = i * train_size\n",
    "    end_index = start_index + train_size\n",
    "    X_trains.append(X_scaled[start_index:end_index, :])\n",
    "    y_trains.append(Y.iloc[start_index:end_index])\n",
    "\n",
    "# Datos para prueba\n",
    "X_test = X_scaled[-train_size:, :]\n",
    "y_test = Y.iloc[-train_size:]\n",
    "\n",
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(258, activation='relu', input_shape=(input_shape,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Entrenar y guardar los modelos\n",
    "for i in range(num_models):\n",
    "    model = create_model(X_trains[i].shape[1])\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print()\n",
    "    print(f\"Modelo {i}\")\n",
    "    model.fit(X_trains[i], y_trains[i], epochs=6, batch_size=32, validation_split=0.2)\n",
    "    model.save(f'modelNuevo{i+1}.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de predicciones y su media mediante bagging y evaluarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "Confusion Matrix:\n",
      "[[ 3541     8]\n",
      " [   29 13383]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      3549\n",
      "         1.0       1.00      1.00      1.00     13412\n",
      "\n",
      "    accuracy                           1.00     16961\n",
      "   macro avg       1.00      1.00      1.00     16961\n",
      "weighted avg       1.00      1.00      1.00     16961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar los modelos entrenados\n",
    "models = [load_model(f'modelNuevo{i+1}.h5') for i in range(5)]\n",
    "\n",
    "# Función para hacer predicciones con Bagging\n",
    "def bagging_predict(models, data):\n",
    "    predictions = [model.predict(data) for model in models]\n",
    "    avg_prediction = np.mean(predictions, axis=0)\n",
    "    return avg_prediction\n",
    "\n",
    "# Hacer predicciones con Bagging en los datos de prueba\n",
    "y_pred_bagging = bagging_predict(models, X_test)\n",
    "y_pred_bagging = (y_pred_bagging > 0.5).astype(int)\n",
    "\n",
    "# Evaluar el rendimiento del modelo combinado\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_bagging)\n",
    "class_report = classification_report(y_test, y_pred_bagging)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentación de los datos en distintos modelos y test y evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Siguiendo los estándares de federated larning es como si existiesen varios dispositivos entrenando un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo 1: Separación en 2 modelos y 1 test\n",
      "\n",
      "Entrenando Modelo 1...\n",
      "Guardando Modelo 1...\n",
      "Evaluando Modelo 1...\n",
      "Modelo 1: Pérdida = 0.0396, Precisión = 0.9761\n",
      "\n",
      "Entrenando Modelo 2...\n",
      "Guardando Modelo 2...\n",
      "Evaluando Modelo 2...\n",
      "Modelo 2: Pérdida = 0.0040, Precisión = 0.9985\n",
      "\n",
      "Ejemplo 1: Separación en 5 modelos y 1 test\n",
      "\n",
      "Entrenando Modelo 1...\n",
      "Guardando Modelo 1...\n",
      "Evaluando Modelo 1...\n",
      "Modelo 1: Pérdida = 0.0707, Precisión = 0.9721\n",
      "\n",
      "Entrenando Modelo 2...\n",
      "Guardando Modelo 2...\n",
      "Evaluando Modelo 2...\n",
      "Modelo 2: Pérdida = 0.0284, Precisión = 0.9961\n",
      "\n",
      "Entrenando Modelo 3...\n",
      "Guardando Modelo 3...\n",
      "Evaluando Modelo 3...\n",
      "Modelo 3: Pérdida = 0.0357, Precisión = 0.9876\n",
      "\n",
      "Entrenando Modelo 4...\n",
      "Guardando Modelo 4...\n",
      "Evaluando Modelo 4...\n",
      "Modelo 4: Pérdida = 0.0148, Precisión = 0.9971\n",
      "\n",
      "Entrenando Modelo 5...\n",
      "Guardando Modelo 5...\n",
      "Evaluando Modelo 5...\n",
      "Modelo 5: Pérdida = 0.0183, Precisión = 0.9977\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(filepath, target_column):\n",
    "    df = pd.read_csv(filepath)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    Y = df[target_column]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, Y\n",
    "\n",
    "def split_data(X_scaled, Y, num_models, train_size):\n",
    "    X_trains = []\n",
    "    y_trains = []\n",
    "    for i in range(num_models):\n",
    "        start_index = i * train_size\n",
    "        end_index = start_index + train_size\n",
    "        X_trains.append(X_scaled[start_index:end_index, :])\n",
    "        y_trains.append(Y.iloc[start_index:end_index])\n",
    "    X_test = X_scaled[-train_size:, :]\n",
    "    y_test = Y.iloc[-train_size:]\n",
    "    return X_trains, y_trains, X_test, y_test\n",
    "\n",
    "def create_model(input_shape):\n",
    "    # Definir la capa de entrada como un objeto Input\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # Definir las capas del modelo\n",
    "    x = Dense(258, activation='relu')(inputs)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Capa de salida\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Crear el modelo\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_save_models(X_trains, y_trains, X_test, y_test, num_models, epochs, batch_size, model_prefix):\n",
    "    metrics = []\n",
    "    for i in range(num_models):\n",
    "        model = create_model(X_trains[i].shape[1])\n",
    "        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        print(f\"\\nEntrenando Modelo {i + 1}...\")\n",
    "        model.fit(X_trains[i], y_trains[i], epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "        print(f\"Guardando Modelo {i + 1}...\")\n",
    "        save_model(model, f'{model_prefix}{i + 1}.keras')  # Guardar en formato Keras nativo\n",
    "        print(f\"Evaluando Modelo {i + 1}...\")\n",
    "        loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        metrics.append((loss, accuracy))\n",
    "        print(f\"Modelo {i + 1}: Pérdida = {loss:.4f}, Precisión = {accuracy:.4f}\")\n",
    "    return metrics\n",
    "\n",
    "def run_example(filepath, target_column, num_models, train_sizes, epochs, batch_size, model_prefix):\n",
    "    X_scaled, Y = load_and_preprocess_data(filepath, target_column)\n",
    "    metrics = []\n",
    "    for i, train_size in enumerate(train_sizes):\n",
    "        print(f\"\\nEjemplo {i + 1}: Separación en {num_models[i]} modelos y 1 test\")\n",
    "        X_trains, y_trains, X_test, y_test = split_data(X_scaled, Y, num_models[i], train_size)\n",
    "        example_metrics = train_and_save_models(X_trains, y_trains, X_test, y_test, num_models[i], epochs[i], batch_size[i], model_prefix[i])\n",
    "        metrics.append(example_metrics)\n",
    "    return metrics\n",
    "\n",
    "# Definición de parámetros para cada ejemplo\n",
    "examples_params = [\n",
    "    {\n",
    "        'num_models': [2],\n",
    "        'train_sizes': [40706],\n",
    "        'epochs': [5],\n",
    "        'batch_size': [32],\n",
    "        'model_prefix': ['model']\n",
    "    },\n",
    "    {\n",
    "        'num_models': [5],\n",
    "        'train_sizes': [16961],\n",
    "        'epochs': [5],\n",
    "        'batch_size': [32],\n",
    "        'model_prefix': ['modelNuevo']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ejecutar cada ejemplo\n",
    "filepath = 'processed_data.csv'\n",
    "target_column = 'diabetesMed_Yes'\n",
    "\n",
    "for params in examples_params:\n",
    "    run_example(filepath, target_column, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step\n",
      "\u001b[1m531/531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step\n",
      "Confusion Matrix:\n",
      "[[ 3543     6]\n",
      " [    6 13406]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      3549\n",
      "         1.0       1.00      1.00      1.00     13412\n",
      "\n",
      "    accuracy                           1.00     16961\n",
      "   macro avg       1.00      1.00      1.00     16961\n",
      "weighted avg       1.00      1.00      1.00     16961\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Función para cargar modelos guardados\n",
    "def load_models(model_prefixes):\n",
    "    models = []\n",
    "    for prefix in model_prefixes:\n",
    "        model = load_model(f'{prefix}.keras')\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "# Función para hacer predicciones con Bagging\n",
    "def bagging_predict(models, data):\n",
    "    predictions = np.zeros((len(data), 1))\n",
    "    for model in models:\n",
    "        prediction = model.predict(data)\n",
    "        predictions += prediction\n",
    "    avg_prediction = predictions / len(models)\n",
    "    return avg_prediction\n",
    "\n",
    "# Ejemplo de cómo usarlo con tus datos y modelos\n",
    "# Suponiendo que 'modelNuevo' es el prefijo para los modelos guardados\n",
    "model_prefixes = [\n",
    "    'modelNuevo1', 'modelNuevo2', 'modelNuevo3', 'modelNuevo4', 'modelNuevo5'\n",
    "]\n",
    "models = load_models(model_prefixes)\n",
    "\n",
    "y_pred_bagging = bagging_predict(models, X_test)\n",
    "y_pred_bagging = (y_pred_bagging > 0.5).astype(int)\n",
    "\n",
    "# Evaluar el rendimiento del modelo combinado\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_bagging)\n",
    "class_report = classification_report(y_test, y_pred_bagging)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
